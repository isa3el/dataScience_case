{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ceeaac8-39c0-4b86-9404-9f6b5389f07f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# iFood Data Science Case: Offer Conversion Prediction\n",
    "\n",
    "This notebook walks through a complete end-to-end workflow to predict whether an iFood offer will be completed by a customer:\n",
    "\n",
    "1. **Data Ingestion & Exploration**  \n",
    "   - Read the processed dataset from Hive  \n",
    "   - Display basic summary and distribution of key variables  \n",
    "\n",
    "2. **Target definition**  \n",
    "   - Create a binary target column (`is_offer_completed`) indicating if an offer was completed  \n",
    "\n",
    "3. **Correlation Analysis**  \n",
    "   - Compute and visualize the Spearman correlation matrix to understand relationships among features  \n",
    "\n",
    "4. **Feature Preparation**  \n",
    "   - Select relevant numeric and categorical features  \n",
    "   - Handle missing values and split into training and test sets  \n",
    "\n",
    "5. **Baseline Modeling with CatBoost**  \n",
    "   - Train a default CatBoostClassifier  \n",
    "   - Evaluate performance via ROC-AUC, classification report and confusion matrix  \n",
    "\n",
    "6. **Hyperparameter Tuning (Optuna)**  \n",
    "   - Define an Optuna study to optimize CatBoost hyperparameters  \n",
    "   - Run trials to maximize ROC-AUC on the hold-out set  \n",
    "\n",
    "7. **Final Model Training & Evaluation**  \n",
    "   - Retrain the optimal CatBoost model with best parameters  \n",
    "   - Assess final performance (ROC-AUC, precision/recall, optimal threshold)  \n",
    "   - Plot feature importances and final confusion matrix and ROC curve \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad492ac1-0202-4308-93b0-1666f3ec3509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install catboost\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc95f5ab-166f-44de-9ec7-c7b1298d35bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from functools import reduce\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PySpark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import (\n",
    "    MultilayerPerceptronClassifier,\n",
    "    RandomForestClassifier as SparkRFClassifier\n",
    ")\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68339d5-a674-47aa-a7ce-53bc62af8272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Data Ingestion & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdd752e3-8ccb-4bbf-bd67-2dc82ce55ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading processed data\n",
    "dataset = spark.table(\"train_df_json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d190ad4b-d6e3-47f1-bac9-98d10af15c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d185dd1d-b354-443c-a785-6e60427667f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Target definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a289e8ce-5a03-4a45-bef7-4d1a744d804c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new binary column 'is_offer_completed' to be target variable\n",
    "dataset = dataset.withColumn(\n",
    "    \"is_offer_completed\",\n",
    "    when(col(\"n_offers_completed\") > 0, 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b023fc1-1a6d-468e-9e6d-d0296a2a3988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "416c396a-4cdb-4a91-99c2-bacb763527b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_pd = dataset.toPandas()\n",
    "\n",
    "# Compute the Spearman correlation matrix between all numeric columns\n",
    "corr = dataset_pd.corr(method='spearman')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdBu_r\",  # Color map: red for positive, blue for negative\n",
    "    center=0,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Correlação'}\n",
    ")\n",
    "\n",
    "plt.title(\"Correlação (spearman)\", fontsize=16, weight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a1b88d-e37f-4ddc-9aa4-b298afcfbef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223bbd27-b02d-4c8d-b8dd-11b49b26a22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define target column\n",
    "target = \"is_offer_completed\"\n",
    "\n",
    "\n",
    "#  Define the list of input features to be used by the model.\n",
    "features = [\n",
    "    \"n_transactions\", \"n_offers_received\", \"n_offers_viewed\", \"avg_amount\", \"spending_ratio\", \"age\", \"credit_card_limit\", \"min_value\", \"discount_value\", \"duration\", \"offer_id\"\n",
    "]\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test) with a fixed seed for reproducibility\n",
    "train_df, test_df = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "X_train = train_df.select(features).toPandas()\n",
    "X_test = test_df.select(features).toPandas()\n",
    "\n",
    "\n",
    "# Extract the target values for training and test flatten the array\n",
    "y_train = train_df.select(target).toPandas().values.ravel()\n",
    "y_test = test_df.select(target).toPandas().values.ravel()\n",
    "\n",
    "# Preparing full dataset for final run\n",
    "full_df = dataset.select(features).toPandas()\n",
    "full_target_df = dataset.select(target).toPandas().values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2befddf2-0873-4850-bd38-d418bc66c15f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify the categorical columns based on data types (object or category)\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Fill missing values in numeric columns with -999\n",
    "# CatBoost can handle missing values, but filling is more consistent\n",
    "X_train.fillna(-999, inplace=True)\n",
    "X_test.fillna(-999, inplace=True)\n",
    "full_df.fillna(-999, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317171cd-51fb-4d78-9423-449401955797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Baseline Modeling with CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1bf54eca-de97-4595-9c77-b22fd6c682a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,         # Number of boosting rounds (trees)\n",
    "    learning_rate=0.1,      # Step size shrinkage used to prevent overfitting\n",
    "    depth=4,                # Maximum depth of each tree\n",
    "    eval_metric='AUC',      # Evaluation metric: Area Under the ROC Curve\n",
    "    verbose=100             # Print training progress every 100 iterations\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "# 'cat_features' specifies which columns are categorical\n",
    "# 'eval_set' is used to monitor performance on the test set during training\n",
    "# 'plot=True' shows a live plot of the learning curve (requires Jupyter environment)\n",
    "model.fit(X_train, y_train, cat_features=categorical_cols, eval_set=(X_test, y_test), plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b6978f-1f79-4117-b7fd-47baac93263c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate class predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Generate predicted probabilities for the positive class (class “1”)\n",
    "# predict_proba returns an array [P(class=0), P(class=1)] for each sample\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Print a detailed classification report (precision, recall, f1-score, support)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Compute and print the ROC AUC score, which measures how well the model separates the two classes across all possible thresholds\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20b6f2c-9ece-4875-9571-ce10b3c733ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix from true labels (y_test) and predictions (y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Reds\")\n",
    "plt.xlabel(\"Predict\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# True Negatives: 5,258 cases where the model correctly did not recommend the offer to customers who did not convert.\n",
    "# False Positives: 1,366 times it suggested the offer to non-converters (“bothering” uninterested customers).\n",
    "# False Negatives: 434 missed converters (the model did not recommend to customers who would have bought).\n",
    "# True Positives: 47,301 cases where it correctly recommended the offer to customers who converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362c8d19-e5e3-4826-8fc9-189341be6d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using Optuna hyperparameter optimization for the CatBoostClassifier,\n",
    "# using ROC AUC on the held-out test set as the objective to maximize\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"iterations\":      trial.suggest_int(\"iterations\",  200, 500),\n",
    "        \"depth\":           trial.suggest_int(\"depth\",       3,    6),\n",
    "        \"learning_rate\":   trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"l2_leaf_reg\":     trial.suggest_float(\"l2_leaf_reg\",   1,    10),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.1, 10),\n",
    "        \"border_count\":    trial.suggest_int(\"border_count\",  32,   255),\n",
    "        \"eval_metric\":     \"AUC\",        \n",
    "        \"loss_function\":   \"Logloss\",    \n",
    "        \"verbose\":         0             \n",
    "    }\n",
    "\n",
    "    #Initialize a CatBoostClassifier with the sampled hyperparameters\n",
    "    model = CatBoostClassifier(**params)\n",
    "\n",
    "    # Train on the training set, validating on the test set\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        cat_features=categorical_cols\n",
    "    )\n",
    "\n",
    "    # Predict probabilities of the positive class on the test set\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Compute and return the ROC AUC score (the objective to maximize)\n",
    "    return roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Create an Optuna study configured to maximize the objective\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Run the optimization for a fixed number of trials. \n",
    "# # Currently using 2 for a quick test; for a full run, set bigger n_trials.\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "# Print out the best AUC and corresponding hyperparameters\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best hyperparameters:\")\n",
    "for name, value in study.best_params.items():\n",
    "    print(f\"  {name}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6676d4ea-4271-4d4a-8a5d-c4d7d0f5695a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Hyperparameter Tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446d0af9-ccbc-4eee-94d9-b108b3a0ebeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get feature importance scores from the trained model\n",
    "importances = model.get_feature_importance()\n",
    "feat_imp_df = pd.DataFrame({'feature': features, 'importance': importances})\n",
    "feat_imp_df.sort_values(by='importance', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1238c88f-cc8b-4a0f-9d1e-a16e3c582238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Final Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d246fe-204c-4e3b-ac54-7af3d3e2d5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the best hyperparameters found by Optuna\n",
    "best_params = study.best_params\n",
    "\n",
    "final_model = CatBoostClassifier(\n",
    "    **best_params,\n",
    "    cat_features=categorical_cols,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "# Train the final model on full data\n",
    "final_model.fit(full_df, full_target_df, eval_set=(full_df, full_target_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be18fbd-01fc-40d6-a186-31dcc642e316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "full_pred_proba = final_model.predict_proba(full_df)[:,1]\n",
    "full_df[\"score\"] = full_pred_proba\n",
    "print(\"ROC AUC:\", roc_auc_score(full_target_df, full_pred_proba))\n",
    "\n",
    "# classificação usando threshold 0.9 \n",
    "full_pred = (full_pred_proba >= 0.9).astype(int)\n",
    "print(classification_report(full_target_df, full_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5f59b2-8360-4ca3-b3aa-b68fad37ebe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotting and ranking feature importance\n",
    "fi = pd.Series(final_model.get_feature_importance(), index=X_train.columns)\n",
    "fi_sorted = fi.sort_values()\n",
    "\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.8, len(fi_sorted)))\n",
    "fig, ax = plt.subplots(figsize=(6,8))\n",
    "ax.barh(fi_sorted.index, fi_sorted.values, color=colors)\n",
    "ax.set_xlabel(\"Importância\")\n",
    "ax.set_title(\"Importância das Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da70378a-055d-43c9-9a2a-e2537618926b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute confusion matrix: rows=true labels, columns=predicted labels\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "\n",
    "# Plot confusion matrix with a red gradient\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot(ax=ax, cmap='Reds', colorbar=False)  # set colorbar=True to show the legend\n",
    "ax.set_title(\"Confusion Matrix (cmap=Reds)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e4c1d4-e85c-4a9c-ba4b-7e14d331284f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, thresholds = roc_curve(full_target_df, full_pred_proba)\n",
    "\n",
    "# Calculate the AUC value\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Find the index of the optimal threshold (maximizes TPR – FPR)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_fpr = fpr[optimal_idx]\n",
    "optimal_tpr = tpr[optimal_idx]\n",
    "\n",
    "# Compute percentage of cases captured at the optimal point\n",
    "percent_aproveitadas = optimal_tpr * 100\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label='ROC Curve', color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random-chance diagonal\n",
    "\n",
    "# Highlight the optimal point\n",
    "plt.scatter(optimal_fpr, optimal_tpr, marker='o', color='darkred', label='Optimal Point')\n",
    "plt.axvline(x=optimal_fpr, linestyle='--', color='darkred')\n",
    "plt.axhline(y=optimal_tpr, linestyle='--', color='darkred')\n",
    "\n",
    "plt.title(\"ROC Curve AUC\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Output the percentage of opportunities seized\n",
    "print(f\"Percent of opportunities seized at optimal threshold: {percent_aproveitadas:.2f}%\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd78eee-8a9c-4b7b-8ea3-37f4d73597c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_pd = dataset.toPandas()\n",
    "full_df[\"account_id\"] = dataset_pd[\"account_id\"]\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9bf1b40-706d-4ef8-9888-652dfef8027d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluated_pairs = (\n",
    "    dataset\n",
    "      .select(\"account_id\", \"offer_id\")   # pick the two columns\n",
    "      .distinct()                         # drop duplicates\n",
    "      .count()                            # count the resulting rows\n",
    ")\n",
    "\n",
    "# 2) How many of those exceed our cutoff (i.e. would be recommended)?\n",
    "threeshoulder = 0.4\n",
    "recommended_pairs = (\n",
    "                     full_df[full_df[\"score\"] > threeshoulder]\n",
    "                    .drop_duplicates([\"account_id\", \"offer_id\"])\n",
    "                    .shape[0]\n",
    "                    )\n",
    "\n",
    "users_reached = full_df.loc[full_df[\"score\"] > threeshoulder, \"account_id\"].nunique()\n",
    "\n",
    "total_users           = full_df[\"account_id\"].nunique()\n",
    "total_offers          = full_df[\"offer_id\"].nunique()\n",
    "total_possible_pairs  = total_users * total_offers\n",
    "\n",
    "total_possible_pairs    = total_users * total_offers\n",
    "evaluated_pairs     = evaluated_pairs    / total_possible_pairs\n",
    "users_reached        = users_reached      / total_users\n",
    "opportunity_captured = recommended_pairs  / total_possible_pairs\n",
    "\n",
    "# Print key opportunity metrics at the chosen cutoff\n",
    "print(f\"Total combinations possibilities:       {total_possible_pairs}\")\n",
    "print(f\"Pairs evaluated out of all possible:       {evaluated_pairs:.2%}\")\n",
    "\n",
    "print(f\"Share of users reached with ≥T recommendation: {users_reached:.2%}\")  \n",
    "print(f\"Overall opportunity captured:               {opportunity_captured:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b90c95c-0804-4029-ab94-d7c223006ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_modeling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
